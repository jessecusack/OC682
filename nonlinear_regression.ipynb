{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Nonlinear Regression\n",
    "\n",
    "How do we estimate the parameters for a nonlinear regression problem? For example, suppose we have data that we believe are represented by a power law with two parameters, e.g.\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}(t) = \\hat{\\beta}_1 t^{\\hat{\\beta}_2}.\n",
    "\\end{equation}\n",
    "\n",
    "This model is not linear in the parameters since the derivative of $\\hat{y}$ with respect to $\\hat{\\beta}_1$ depends on $\\hat{\\beta}_2$. Morover, the derivative with respect of $\\hat{\\beta}_2$ depends on both $\\hat{\\beta}_1$ and $\\hat{\\beta}_2$. \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\hat{y}}{\\partial \\hat{\\beta}_1} = & t^{\\hat{\\beta}_2} \\\\\n",
    "\\frac{\\partial \\hat{y}}{\\partial \\hat{\\beta}_2} = & \\hat{\\beta}_1 t^{\\hat{\\beta}_2} \\log(t) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Power law models like this are often used to describe power spectra of ocean variables such as velocity and temperature. For example, the energy of some processes are postulated to have a frequency dependence of $\\omega^{-2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorednoise as cn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.signal as sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "We can generate synthetic data with a -2 spectral slope using the `colorednoise` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 2**10\n",
    "t, dt = np.linspace(0, 100, n_data, retstep=True)\n",
    "u = cn.powerlaw_psd_gaussian(2, n_data, random_state=1351)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, u)\n",
    "ax.set_xlabel(\"t\")\n",
    "ax.set_ylabel(\"u(t)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "The power spectrum may have a -2 dependence. It is difficult to see with a linear axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "om, Pu = sig.welch(u, fs=1 / dt, window=\"hann\", nperseg=256, noverlap=128)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(om, Pu)\n",
    "ax.set_xlabel(\"$\\omega$\")\n",
    "ax.set_ylabel(\"$P_u(\\omega)$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Transforming to a log-log axis makes the power law much more obvious. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.loglog(om, Pu)\n",
    "ax.set_xlabel(r\"$\\omega$\")\n",
    "ax.set_ylabel(r\"$P_u(\\omega)$\")\n",
    "\n",
    "f_guide = np.array([1e0, 2e0])\n",
    "ax.loglog(f_guide, f_guide ** (-2), \"k--\")\n",
    "ax.annotate(fr\"$\\omega^{-2}$\", xy=(f_guide[0], f_guide[0] ** (-2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Plotting on logarithmic axes is equivalet to transforming the model by taking the logarithm. \n",
    "\n",
    "\\begin{equation}\n",
    "\\log(\\hat{y}(t)) = \\log(\\hat{\\beta}_1) + \\hat{\\beta}_2 \\log(t)\n",
    "\\end{equation}\n",
    "\n",
    "In this case, taking the logarithm transforms the problem into a linear regression problem. We could rewrite the equation above as\n",
    "\n",
    "\\begin{equation}\n",
    "z = b + m x \n",
    "\\end{equation}\n",
    "\n",
    "where $z = \\log(\\hat{y}(t))$, $b = \\log(\\hat{\\beta}_1)$, $m=\\hat{\\beta}_2$ and $x = \\log(t)$.  It is now the univariate 2-parameter linear model. The parameters can be estimated using the linear regression techniques seen before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack((np.ones_like(om[1:]), np.log(om[1:])))\n",
    "log_params = np.linalg.lstsq(X, np.log(Pu[1:]), rcond=None)[0]\n",
    "\n",
    "beta_1 = np.exp(log_params[0])\n",
    "beta_2 = log_params[1]\n",
    "y_log_fit = beta_1 * om[1:]**beta_2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.loglog(om[1:], Pu[1:])\n",
    "ax.loglog(om[1:], y_log_fit, \"k\", label=f\"${beta_1:.3f} \\omega^{{{beta_2:.2f}}}$\")\n",
    "ax.set_xlabel(\"$\\omega$\")\n",
    "ax.set_ylabel(\"$P_u(\\omega)$\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "One key assumption in this transformation is that the errors are multiplicative, not additive. The linear regression model we saw before had additive errors, e.g. $y = \\hat{y} + \\epsilon$, thus allowing us to minimize $\\langle \\epsilon^2 \\rangle = \\langle (y - \\hat{y})^2 \\rangle$. Taking the logarithm of the addative error model impedes our ability minimize the error. How can we rearrange $\\log(\\hat{y} + \\epsilon)$ to minimize $\\epsilon$? If the errors are multiplicative, meaning $y = \\epsilon \\hat{y}$, then taking the logarithm produces an additive error term that can be minimized.  \n",
    "\n",
    "Another issue when applying linear regression to the logarithm of data is the asymmetric weighting of data in the fit. Small values are given more weight. In other words, you may not fit large values well. A model difference of 1 in $\\log_{10}$ space is equivalent to a multiplicative difference of $10 \\times$ in linear space. Thus, a misfit of 1 in $\\log_{10}$ at a y data point that has a value of 100 could mean a difference of $10 \\times 100 - 100 = 900$ or $100/10 - 100 = -90$. The same $\\log_{10}$ difference of 1 on data with value 1 could mean a difference of +9 or -0.9. Much smaller! Be wary. \n",
    "\n",
    "What about a nonlinear model that cannot be reduced to a linear problem? E.g. $\\hat{y}(t) = \\hat{\\beta}_1 \\cos(\\hat{\\beta}_2 t)$. Or what if we don't want to reduce our power law to a linear regression problem because we are not sure that the assumptions hold? Then we need to use nonlinear regression. \n",
    "\n",
    "Minimizing the mean square error $\\langle \\epsilon^2 \\rangle$ is still the main goal of nonlinear regression. However, now we must take a new approach. We are not able to derive an analytical solution, so we rely on a numerical solution. \n",
    "\n",
    "## Nonlinear Least Squares\n",
    "\n",
    "Several methods are available to minimize a nonlinear least squares problem. We will consider the Gauss-Newton algorithm. The method relies on a first guess of the parameters, followed by successive iterations to hone in on the true values. The goal is to estimate all the $M$ parameters,\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\hat{\\beta}} =\n",
    "\\begin{bmatrix}\n",
    "\\hat{\\beta}_1 \\\\\n",
    "\\hat{\\beta}_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{\\beta}_M\n",
    "\\end{bmatrix},\n",
    "\\end{equation}\n",
    "\n",
    "by minimizing the square error between the data and the model. At each iteration $k$ we update our initial parameter guess by some amount $\\Delta \\boldsymbol{\\hat{\\beta}}$ to use in the next iteration $k+1$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\hat{\\beta}}^{k+1} = \\boldsymbol{\\hat{\\beta}}^k + \\Delta \\boldsymbol{\\hat{\\beta}}^k\n",
    "\\end{equation}\n",
    "\n",
    "The iteration ends when we are satisfied that we are close to the best parameter estimate. How do we find $\\Delta \\boldsymbol{\\hat{\\beta}}^k$?\n",
    "\n",
    "First, we recognize that the model is a function of the independent variable at times $n = 1, 2, ... N$, and the parameters, e.g. $\\hat{y} = \\hat{y}(t_n, \\boldsymbol{\\hat{\\beta}})$. Because the model is nonlinear it cannot be written as a matrix of inputs multiplying a matrix of parameters. However, we can linearize the model about the parameters using a Taylor expansion,.\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}(t_n, \\boldsymbol{\\hat{\\beta}}^k + \\Delta \\boldsymbol{\\hat{\\beta}}^k) \\approx \\hat{y}(t_n, \\boldsymbol{\\hat{\\beta}}^k) + \\boldsymbol{J} \\Delta\\boldsymbol{\\hat{\\beta}}^k,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\boldsymbol{J}(t_n, \\boldsymbol{\\hat{\\beta}}^k)$ is the $N\\times M$ Jacobian matrix,\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{J} =\n",
    "\\begin{bmatrix}\n",
    "\\dfrac{\\partial \\hat{y}(t_1, \\boldsymbol{\\hat{\\beta}}^k)}{\\partial \\hat{\\beta}_1} & \\dfrac{\\partial \\hat{y}(t_1, \\boldsymbol{\\hat{\\beta}}^k)}{\\partial \\hat{\\beta}_2} & \\cdots & \\dfrac{\\partial \\hat{y}(t_1, \\boldsymbol{\\hat{\\beta}}^k)}{\\partial \\hat{\\beta}_M} \\\\[1.5em]\n",
    "\\dfrac{\\partial \\hat{y}(t_2, \\boldsymbol{\\hat{\\beta}}^k)}{\\partial \\hat{\\beta}_1} & \\dfrac{\\partial \\hat{y}(t_2, \\boldsymbol{\\hat{\\beta}}^k)}{\\partial \\hat{\\beta}_2} & \\cdots & \\dfrac{\\partial \\hat{y}(t_2, \\boldsymbol{\\hat{\\beta}}^k)}{\\partial \\hat{\\beta}_M} \\\\[1.5em]\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\[1.5em]\n",
    "\\dfrac{\\partial \\hat{y}(t_N, \\boldsymbol{\\hat{\\beta}}^k)}{\\partial \\hat{\\beta}_1} & \\dfrac{\\partial \\hat{y}(t_N, \\boldsymbol{\\hat{\\beta}}^k)}{\\partial \\hat{\\beta}_2} & \\cdots & \\dfrac{\\partial \\hat{y}(t_N, \\boldsymbol{\\hat{\\beta}}^k)}{\\partial \\hat{\\beta}_M}\n",
    "\\end{bmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "We are trying to minimize\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle (y - \\hat{y})^2 \\rangle.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
